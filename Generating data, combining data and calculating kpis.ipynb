{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "Below we create a file that determines which simulations should be ran, for which graphs, and how often. It exports a .sh file that can be executed in the terminal. To run the code and the bash file, ensure that all the folders to store data exist locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "summary_folder = Path(\"data/public_release/varying_symptom_rate\")\n",
    "\n",
    "\n",
    "records = [{\"app_rate\": np.round(app_rate, 6),\n",
    "            \"beta\": np.round(beta, 2),\n",
    "            \"graph\": graph,\n",
    "            \"method\": method,\n",
    "            \"symptom_prob\": np.round(symptom_prob,2),\n",
    "            \"runs_to_go\": 5}                                     \n",
    "            for beta in [0.2]\n",
    "            for symptom_prob in [0.02]\n",
    "            for app_rate in [0.2, 0.4, 0.6, 0.8]\n",
    "            for method in [\n",
    "                \"recommender\", \n",
    "                           \"recommend_friends_with_p=0.5\",\n",
    "                           \"degree\",\n",
    "                           \"random\"\n",
    "            ]\n",
    "            for graph in [\n",
    "                \"girgC_tau=2.3_alpha=1.3_deg=13.tar\",\n",
    "#                 \"girgC_tau=2.3_alpha=2.3_deg=13.tar\",\n",
    "#                 \"girgC_tau=3.3_alpha=1.3_deg=13.tar\",\n",
    "#                 \"girgC_tau=3.3_alpha=2.3_deg=13.tar\",\n",
    "#                 \"cm_tau=2.3_deg=13.tar\",\n",
    "#                 \"cm_tau=3.3_deg=13.tar\"\n",
    "            ]]\n",
    "records += [{\"app_rate\": np.round(app_rate, 6),\n",
    "            \"beta\": np.round(beta, 2),\n",
    "            \"graph\": graph,\n",
    "            \"method\": method,\n",
    "            \"symptom_prob\": np.round(symptom_prob,2),\n",
    "            \"runs_to_go\": 5}                                     \n",
    "            for beta in [0.2]\n",
    "            for symptom_prob in [0.02]\n",
    "            for app_rate in [0, 1]\n",
    "            for method in [\n",
    "#                 \"recommender\", \n",
    "#                 \"recommend_friends_with_p=0.5\",\n",
    "#                 \"degree\",\n",
    "                  \"random\"\n",
    "            ]\n",
    "            for graph in [\n",
    "                \"girgC_tau=2.3_alpha=1.3_deg=13.tar\",\n",
    "                \"girgC_tau=2.3_alpha=2.3_deg=13.tar\",\n",
    "#               \"girgC_tau=3.3_alpha=1.3_deg=13.tar\",\n",
    "#               \"girgC_tau=3.3_alpha=2.3_deg=13.tar\",\n",
    "#               \"cm_tau=2.3_deg=13.tar\",\n",
    "#               \"cm_tau=3.3_deg=13.tar\"\n",
    "            ]]\n",
    "\n",
    "run_df = pd.DataFrame.from_records(records)\n",
    "coverages_df = pd.concat([pd.read_csv(\"optimized_coverages_cm.csv\"), pd.read_csv(\"optimized_coverages_girgC.csv\")])\n",
    "run_df[\"target_coverage\"] = np.round(run_df[\"app_rate\"], 2)\n",
    "coverages_df[\"graph\"] = coverages_df[\"graph\"].str[:-3]\n",
    "coverages_df[\"target_coverage\"] = np.round(coverages_df[\"target_coverage\"], 2)\n",
    "index_cols = [\"graph\", \"method\", \"target_coverage\"]\n",
    "run_df = run_df.set_index(index_cols).join(coverages_df.set_index(index_cols), rsuffix='_initial').reset_index()\n",
    "run_df[\"coverage\"] = run_df[\"coverage\"].fillna(run_df[\"app_rate\"])\n",
    "run_df[\"app_rate_initial\"] = np.round(run_df[\"app_rate_initial\"].fillna(run_df[\"app_rate\"]), 6)\n",
    "run_df[\"app_rate_initial_rounded\"] = np.round(run_df[\"app_rate_initial\"], 3)\n",
    "\n",
    "    \n",
    "def blow_up(record):    \n",
    "    runs_to_go = record.pop(\"runs_to_go\")    \n",
    "    return [{**record, \"run_id\": i}  for i in range(1, runs_to_go + 1)]\n",
    "run_df = pd.DataFrame.from_records([new_record \n",
    "                                   for old_record in run_df[run_df[\"runs_to_go\"]>0].to_dict(orient=\"record\") \n",
    "                                   for new_record in blow_up(old_record)])\n",
    "\n",
    "\n",
    "records_todo = run_df.to_dict(orient=\"record\")\n",
    "\n",
    "record_to_line = lambda record: \\\n",
    "f\"sem -j+0 \\\"python varying_app_rate.py \\\n",
    "data/graphs-christopher/{record['graph']}.gz \\\n",
    "{record['beta']} \\\n",
    "{record['symptom_prob']} \\\n",
    "{np.round(record['app_rate_initial'], 6)} \\\n",
    "{record['method']} \\\n",
    "data/public_release/varying_symptom_rate/{record['graph']}/{record['method']}/run={record['run_id']}_beta={record['beta']}_app_rate={np.round(record['app_rate_initial'],6)}_symptom_prob={record['symptom_prob']}#$(date +%s).tar.gz\\\"\" \n",
    "\n",
    "\n",
    "    \n",
    "lines = ([\"#!/bin/sh\", \"set -e\", \"\\n\"] + \n",
    "         [record_to_line(record) \n",
    "          for record in run_df.to_dict(orient=\"record\")]\n",
    "         + [\"\\n\", \"sem --wait\"]\n",
    "         )\n",
    "\n",
    "with open(\"app-test.sh\", \"w\") as f:\n",
    "    f.writelines(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining data\n",
    "Below we read the generated data by the bash script and combine the results into a single large csv that can be read by the \"Visualisations paper\" notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = list(Path(\"data/public_release/varying_symptom_rate\").glob(\"g*/*/s*.tar.gz\"))\n",
    "\n",
    "def read_path(summary_path):\n",
    "    try:\n",
    "        df = SpreadingStateHistory.load_bundled_summaries_to_time_df(summary_path,\n",
    "                                                                     bundle_id=summary_path.name,\n",
    "                                                                     description_params=[\"method\",\n",
    "                                                                                         \"symptom_prob\",\n",
    "                                                                                         \"infection_prob\",\n",
    "                                                                                         \"average_degree\",\n",
    "                                                                                         \"app_coverages\",\n",
    "                                                                                         \"app_rate\"])\n",
    "        df[\"graph\"] = summary_path.parent.parent.name\n",
    "        df[\"Run_ID\"] = df[\"Run_ID\"] + \"__\" + df[\"Bundle_ID\"] \n",
    "        df[\"app_method\"] = df[\"method\"]\n",
    "    except:\n",
    "        raise\n",
    "        df = pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "new_dfs = [read_path(summary_path) for summary_path in paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(new_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrich data\n",
    "We enrich the data by rounding the coverage off the app, adding extra columns with different names for the strategy, computing hospital load, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_full_data(df):\n",
    "    # In case of no/full app usage all systems coincide and results can be reused across scenarios\n",
    "    mask = df[\"app_rate\"].isin([0, 1]) & (df[\"app_method\"]!=\"random\")\n",
    "    df = df[~mask]\n",
    "    trivial_df = df[df[\"app_rate\"].isin([0,1]) & (df[\"app_method\"]==\"random\")].copy()\n",
    "    for method in set(df[\"app_method\"].unique()) - {\"random\"}:\n",
    "        trivial_df[\"app_method\"] = method\n",
    "        trivial_df[\"method\"] = method\n",
    "        df = pd.concat([df, trivial_df])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # the precise app_coverage for recommender systems are determined by initial app users that differ per run\n",
    "    # to combine different runs with 'almost' same app_coverage two columns are added\n",
    "    df.drop(columns=[c for c in df.columns if c.startswith(\"app_coverages_\")], inplace=True)\n",
    "    group_cols = [\"app_method\", \"graph\", \"app_rate\", \"symptom_prob\", \"infection_prob\"]\n",
    "    df = df.set_index(group_cols).join(df.groupby(group_cols)[[\"app_coverages\"]].mean(), rsuffix=\"_mean\").reset_index()\n",
    "    mask = ((df[\"app_coverages_mean\"] % 0.05) > 0.025) \n",
    "    df[\"app_coverages_rounded\"] = df[\"app_coverages_mean\"] - (df[\"app_coverages_mean\"] % 0.05) \n",
    "    df.loc[mask, \"app_coverages_rounded\"] = df[\"app_coverages_rounded\"] + 0.05  \n",
    "    df[\"app_coverages_rounded\"] = np.round(df[\"app_coverages_rounded\"], 2)\n",
    "    # scale by 100 to display percentages instead of fractions\n",
    "    df[\"app_coverages_mean_perc\"] = 100*df[\"app_coverages_mean\"]\n",
    "\n",
    "\n",
    "    ########\n",
    "    df[\"Quarantined\"] = (df[\"Q_susceptible\"] \n",
    "                         + df[\"Q_exposed\"] \n",
    "                         + df[\"Q_infected\"] \n",
    "                         + df[\"Symptomatic\"] \n",
    "                         + df[\"Q_removed\"])\n",
    "    df[\"Newly Quarantined\"] = (df[\"Newly Q_susceptible\"] \n",
    "                                 + df[\"Newly Q_exposed\"] \n",
    "                                 + df[\"Newly Q_infected\"] \n",
    "                                 + df[\"Newly Symptomatic\"] \n",
    "                                 + df[\"Newly Q_removed\"])\n",
    "\n",
    "    df[\"Strategy\"] = (df[\"app_method\"]\n",
    "                      .replace(\"recommender\", \"Basic Recommender\")\n",
    "                      .replace(\"recommend_friends_with_p=0.5\", \"Ring Recommender\")\n",
    "                      .replace(\"random\", \"Random\")\n",
    "                      .replace(\"degree\", \"Degree\"))\n",
    "    df[\"Hospital\"] = 0.05 * df[\"Symptomatic\"] / df[\"symptom_prob\"]\n",
    "    df = df.sort_values(by=[\"graph\",\n",
    "                            \"app_method\", \n",
    "                            \"infection_prob\", \n",
    "                            \"app_coverages_mean\", \n",
    "                            \"symptom_prob\",\n",
    "                            \"Run_ID\",\n",
    "                            \"Time\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_df = enrich_full_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute KPIs\n",
    "From this data we can extract the KPIs as presented in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kpi_df(df, upper_percentile=90, lower_percentile=10):\n",
    "    df = df.copy()\n",
    "    index_cols_setting = [\"app_rate\", \n",
    "                          \"app_coverages_mean\",\n",
    "                          \"app_coverages_rounded\",\n",
    "                          \"app_coverages_mean_perc\",\n",
    "                          \"Strategy\",\n",
    "                          \"graph\", \n",
    "                          \"infection_prob\", \n",
    "                          \"symptom_prob\"]\n",
    "    index_cols_run = index_cols_setting + [\"Bundle_ID\", \"Run_ID\"]\n",
    "    if upper_percentile < 100:\n",
    "        upper = lambda x: np.percentile(x, upper_percentile)\n",
    "    else:\n",
    "        upper = np.max\n",
    "    if lower_percentile > 0:\n",
    "        lower = lambda x: np.percentile(x, lower_percentile)\n",
    "    else:\n",
    "        lower = np.min\n",
    "        \n",
    "    agg_functions = [\"median\", \n",
    "                     ('upper', upper),\n",
    "                     ('lower', lower)]\n",
    "\n",
    "    def percentage_in_quarantine(df):\n",
    "        a = df.groupby(index_cols_run).agg({\"Quarantined\":\"sum\", \"Time\":\"max\"})\n",
    "        a[\"Quar./person/year\"] = 365 * a[\"Quarantined\"] / (a[\"Time\"] * 500000)\n",
    "        a[\"Quar./person\"] =  a[\"Quarantined\"] / (500000)\n",
    "        return a.reset_index().groupby(index_cols_setting)[[\"Quar./person/year\", \"Quar./person\"]].agg(agg_functions)\n",
    "\n",
    "\n",
    "    \n",
    "    kpi_df = (df.groupby(index_cols_run).agg({\"Removed\":\"max\",\n",
    "                                              \"Symptomatic\": \"max\",\n",
    "                                              \"Quarantined\":\"max\",\n",
    "                                              \"Time\": \"max\",\n",
    "                                              \"Hospital\": \"max\"})\n",
    "                        .reset_index()\n",
    "                        .groupby(index_cols_setting)\n",
    "                        .agg(agg_functions)\n",
    "                        .join(percentage_in_quarantine(df))\n",
    "                        .join(df.groupby(index_cols_run)[[\"Symptomatic\"]]\n",
    "                                .max()\n",
    "                                .reset_index()\n",
    "                                .set_index(index_cols_run + [\"Symptomatic\"])\n",
    "                                .join(df[index_cols_run + [\"Symptomatic\", \"Time\"]].set_index(index_cols_run + [\"Symptomatic\"]), rsuffix=\"_r\")\n",
    "                                .reset_index()\n",
    "                                .groupby(index_cols_setting)[[\"Time\"]]\n",
    "                                .agg(agg_functions)\n",
    "                                .rename(columns={\"Time\": \"Time of peak\"}))\n",
    "                        .rename(columns={\"Symptomatic\": \"Symptomatic Max.\", \n",
    "                                         \"Quarantined\": \"Quarantined Max.\",\n",
    "                                         \"Hospital\": \"Hospital Max.\",\n",
    "                                         \"Removed\": \"Volume\",\n",
    "                                         \"Time\": \"Time to epidemic end\"})\n",
    "                        .reset_index())    \n",
    "    kpi_df[\"Time to epidemic end\"] = kpi_df[\"Time to epidemic end\"]/365\n",
    "    kpi_df = kpi_df.rename(columns={\"Time to epidemic end\": \"Years to epidemic end\"})\n",
    "    kpi_df.columns = [\", \".join(col) if len(col[1])>0 else col[0] \n",
    "                              for col in kpi_df.columns]\n",
    "    percentage_cols = ['Volume, median', 'Volume, upper', 'Volume, lower', \n",
    "                   'Symptomatic Max., median', 'Symptomatic Max., upper', 'Symptomatic Max., lower', \n",
    "                   'Quarantined Max., median', 'Quarantined Max., upper', 'Quarantined Max., lower',  \n",
    "                   'Hospital Max., median', 'Hospital Max., upper', 'Hospital Max., lower']\n",
    "    for col in percentage_cols:\n",
    "        kpi_df[\" (%), \".join(col.split(', '))] = kpi_df[col]/5000\n",
    "    return (kpi_df.set_index(index_cols_setting)\n",
    "                  .join(df.groupby(index_cols_setting)[[\"Run_ID\"]]\n",
    "                          .nunique()\n",
    "                          .rename(columns={\"Run_ID\":\"# Runs\"}))\n",
    "                  .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df = create_kpi_df(enriched_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpi_df.to_csv(\"data/public_release/kpi.csv.gz\", index=False)\n",
    "enriched_df.to_csv(\"data/public_release/data.csv.gz\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-tim",
   "language": "python",
   "name": "env-tim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
